{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb3295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "def recuperar_transacoes(i):\n",
    "    filename = \"transactions_all.txt\"\n",
    "    with open(filename, 'r') as file:\n",
    "        transactions = [line.strip().split() for line in file.readlines()]\n",
    "    return transactions\n",
    "\n",
    "indice_arquivo = 0\n",
    "transactions = recuperar_transacoes(indice_arquivo)\n",
    "\n",
    "\n",
    "filtered_transactions0 = [transaction for transaction in transactions if transaction[-1] == '0']\n",
    "filtered_transactions1 = [transaction for transaction in transactions if transaction[-1] == '1']\n",
    "#add filtered transactionX for any number of classes in need and then put them on the filtered_transactions_list\n",
    "\n",
    "filtered_transactions_list = [filtered_transactions0, filtered_transactions1]\n",
    "\n",
    "\n",
    "def calculate_jaccard_similarity(set1, set2):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    similarity = len(intersection) / len(union)\n",
    "    return similarity\n",
    "\n",
    "def extract_association_rules(transactions, n_classes, rule_params,similar,supt):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = fpgrowth(df, min_support=supt, use_colnames=True)\n",
    "    rulesi = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=0)\n",
    "    keep_indices = []\n",
    "\n",
    "    for idx, antecedents in enumerate(rulesi['antecedents']):\n",
    "        discard_rule = True \n",
    "        yi=[]\n",
    "        for item in antecedents:\n",
    "            if item.startswith('y'):\n",
    "                yi_pattern = item.split('z')[0]\n",
    "                yi.append(yi_pattern)\n",
    "        if yi:\n",
    "            occurrences = {pattern: yi.count(pattern) for pattern in yi}\n",
    "            if not any(count == 1 for count in occurrences.values()):\n",
    "                discard_rule = False\n",
    "        if not discard_rule:\n",
    "            keep_indices.append(idx)\n",
    "            \n",
    "    rules = rulesi.iloc[keep_indices]\n",
    "    \n",
    "    filtered_rules = rules[rules['consequents'].apply(lambda x: len(x) == 1 and isinstance(list(x)[0], str) and list(x)[0].isdigit())]\n",
    "    \n",
    "    new_rules = pd.DataFrame()\n",
    "    for cls, (n_rules, min_antecedent_size) in enumerate(rule_params):\n",
    "        class_rules = filtered_rules[filtered_rules['consequents'].apply(lambda x: str(cls) in x)]\n",
    "        class_rules_with_min_antecedent = class_rules[class_rules['antecedents'].apply(lambda x: len(x) >= min_antecedent_size)]\n",
    "        new_rules = pd.concat([new_rules, class_rules_with_min_antecedent])\n",
    "        \n",
    "    final_rules=pd.DataFrame()\n",
    "    for cls, (n_rules, min_antecedent_size) in enumerate(rule_params):\n",
    "        sim=True\n",
    "        while sim:\n",
    "            same_class_rule = new_rules[new_rules['consequents'].apply(lambda x: str(cls) in x)]\n",
    "            top_same_class_rule = same_class_rule.nlargest(n_rules, 'lift')\n",
    "            rep=[]\n",
    "            for ij, i in enumerate(top_same_class_rule.iterrows()):\n",
    "                if ij not in rep:\n",
    "                    idx_i, rule_i = i\n",
    "                    for jk, j in enumerate(top_same_class_rule.iterrows()):\n",
    "                        if jk > ij and jk not in rep:\n",
    "                            idx_j, rule_j = j\n",
    "                            similarity = calculate_jaccard_similarity(set(rule_i['antecedents']), set(rule_j['antecedents']))\n",
    "                            \n",
    "                            if similarity>=similar:\n",
    "                                rep.append(jk)\n",
    "                                for idx, rule in new_rules.iterrows():\n",
    "                                    if set(rule_j['antecedents']) == set(rule['antecedents']):\n",
    "                                        new_rules = new_rules.drop(idx)\n",
    "            if not rep:\n",
    "                sim=False\n",
    "                final_rules = pd.concat([final_rules, top_same_class_rule])\n",
    "                \n",
    "\n",
    "                                                     \n",
    "        \n",
    "\n",
    "\n",
    "    return final_rules\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "all_result_rules = pd.DataFrame()\n",
    "\n",
    "\n",
    "n_classes = 2 #Number of classes\n",
    "rule_params = [[75, 3], [75, 3]] #rule_params[0]->[maximum number of rules obtained for class 0, \\\n",
    "                                                  #minimum antecedent size for each rules from class 0]\n",
    "similar = 0.3 #Jaccard Similarity amoung all antecedent groups must be below 30%\n",
    "supt=[0.2,0.2] #[min support for class0, min support for class 1]\n",
    "for i, transactions in enumerate(filtered_transactions_list):\n",
    "    result_rules = extract_association_rules(transactions, n_classes, rule_params, similar,supt[i])\n",
    "    \n",
    "    all_result_rules = pd.concat([all_result_rules, result_rules])\n",
    "\n",
    "\n",
    "count_consequents = all_result_rules['consequents'].value_counts()\n",
    "print(count_consequents)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "caminho_arquivo_pickle = 'result_rules0.pkl'\n",
    "rulescsv='result_rules0.csv'\n",
    "\n",
    "all_result_rules.to_pickle(caminho_arquivo_pickle)\n",
    "all_result_rules.to_csv(rulescsv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
